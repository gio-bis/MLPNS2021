{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Extraction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPKWmIUqS0ldy7QaWMvliDC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gio-bis/MLPNS2021/blob/main/Appunti%20Lezioni/Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI84ZFL3HjH9"
      },
      "source": [
        "I tree model trattano una variabile alla volta, quindi l'idea è separare lo spazio con linee parallele agli assi. Questo permette di utilizzare i tree model anche se ho variabili di diverso tipo (categoriche e numeriche).\n",
        "\n",
        "Il **gini index** è la target function che ad ogni nodo massimizzo, quindi scelgo la variabili che mui dà il più alto gini index. Sulla base del fatto che ogni variabile ha un associato gini index e che alcune variabili vengono usate più di una volta posso stabilire l'importanza della variabile. Posso fare un plot in cui l'importanza della variabile varia tra 0 e 1, se è 1 è l'unica variabile che conta, se è 0 non serve a niente. Se uso un random forest posso mettere anche l'incertezza della variabile, che è frequentista (l'importanza di una variabile è una media su tutti gli alberi e l'incertezza ne è la standard deviation). Le cose sono complicate: non sono necessariamente in uno spazio di variabili indipendenti, quindi alberi diversi potrebbero scegliere variabili diverse per ottenere però lo stesso split con stesso gini index. Quindi, quando metto insieme tutti gli alberi, ho un'artificiale soppressione dell'importanza di una delle variabili. La correlazione tra variabili può essere soppressa, per esempio usando uno spazio di variabili relazionato allo spazio originale, ma di variabili indipendenti mediante il PCA (Principal Component Analysis). Tuttavia, dopo aver fatto il pca lavoro in uno spazio di variabili che non è più quello iniziale, quindi sto lavorando in una combinazione lineare delle variabili, che non permette il feature exctraction. Non c'è una soluzione a questo problema.\n",
        "\n",
        "I tree models sono i più semplici per lavorare in features spaces che hanno variabili ibride (sia numeriche che categoriche).\n",
        "\n",
        "Numerical encoding non va bene per i trees perché implica un ordine che non esiste, ma che ho introdotto io. Invece, il problema dell'one-hot encoding è che la dimensione del features space cresce. Inoltre, continuo ad assumere che il features space sia indipendente, ma queste variabili sono correlate perché sono clasi esclusive e io non ho modo di comunicarlo ai modelli ad albero. Questo è però la scelta preferita in letteratura. Si tratta però un problema quando si deve studiare la features importance\n",
        "\n",
        "\n",
        "### Data Visualization\n",
        "I can compress large feature spaces into smaller variables. They are dimensionality reduction models (for example TSNE, che proietta mantenendo le distanze tra oggetti).\n",
        "LO spazio delle features è rappresentato in due dimensioni e i cluster sono di colori diversi\n",
        " "
      ]
    }
  ]
}