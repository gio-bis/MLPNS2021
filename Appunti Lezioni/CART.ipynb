{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CART.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPkowNIuViOn0rNge7qLh+z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gio-bis/MLPNS2021/blob/main/Appunti%20Lezioni/CART.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFq4xGXc_-PY"
      },
      "source": [
        "Un single tree model è una sequenza di decisioni, ciascuna delle quali è binaria. I single tree sono però problematici perché hanno high variance (variabilità del risultato). Siccome per ogni variabile continua c'è un'infinita possibilità di fare uno split, questo introduce un problema computazionale: non si può risolvere il problema esplorando interamente la likelihood surface per ottenere una posterior che sia veramente l'ottimizzazione del problema. Siccome non si può fare una ricerca computazionale completa, si deve introdurre un random guess per inizializzare scelte che rendono il problema computazionalmente possibile. Quindi non si può fare una ricerca esaustiva del modllo e la scelta è basata su ottimizzazione locale, non globale. Questi 2 problemi rendono i single trees praticamente inutilizzabili.\n",
        "La soluzione è far correre il modello più volte e vedere tra tutti i risultati diversi qual è il più comune. Si parla di ensemble methods. Ci sono 2 scelte:\n",
        "\n",
        "1) Random Forests (stochastic variation, alberi in parallelo, indipendenti l'un l'altro, usano un subset di osservabili/features)\n",
        "\n",
        "2) Gradient Boosted Trees (progressive variation, usano tutto lo spazio delle features, ma aggiunge pesi alle features che imparano dagli alberi precedenti; l'ultimo albero è quello che ha la predizione finale)\n",
        "\n",
        "\\\n",
        "Gli alberi possono essere usati anche per fare regression, pensandoli come classifier con classi molto piccole."
      ]
    }
  ]
}