{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TSNE + NN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMWaK+sqm2mgWq2PLM9xyKM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gio-bis/MLPNS2021/blob/main/Appunti%20Lezioni/TSNE_%2B_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgvvLGL4GKN2"
      },
      "source": [
        "A bassa perplexity ( = 3) la proiezione fallisce, mi dice che tutti gli oggetti sono alla stessa distanza, ma so che non è vero perché lo vedo in altre T-SNE e perché è un dataset che può essere classificato sulla base di queste variabili, quindi c'è una struttura. A perplexity = 10 l'exaggeration permette di trovare outlaiers. Nella maggiorparte dei casi faccio raggruppamento proprio per trovare outlaiers. Perplexity = 50 ha creato regioni interessanti (alcune isole).\n",
        "\n",
        "\n",
        "## Neural Newtwork\n",
        "All machine learning (with the exception of lazy learning) relies on setting a loss function. It is a metric that tells me how far my prediction is from my target. \n",
        "\n",
        "We take data in input, we do a simple processing which has just binary operation and we output a number which is originally binary but via some math tricks we can turn it into a non binary output. Neuron are essentially classifier, the can only output binary values. \n",
        "\n",
        "A single neuron which has a binary output is called perceptrons. \n",
        "Se alcune features sono più importanti di altre uso dei pesi. Una volta che inserisco i pesi nel mio perceptron, esso diventa un machine learning model. Posso aggiungere un bias che è qualcosa che mi aiuta a superare la threshold. \n",
        "\n",
        "Il perceptron è un linear classifier.\n",
        "\n",
        "La **funzione di attivazione** è ciò che mi porta dalla threshold $\\theta$ fino a un output binario (0 o 1). Posso scegliere funzioni di attivazione più complicate di una funzione a step. Si tratta di un classifier probabilistico. \n",
        "\n",
        "Per fare cose complicate si devono mettere perceptrons insieme in layers. \n",
        "\n",
        "**Deep Learning** means increasing the number of hidden layers. \n",
        "\n",
        "Si preferisce il modello più semplice allo stesso livello di performance. Più semplice significa con il minor numero possibile di parametri (nella regressione lineare i parametri sono slope e intercept).\n",
        "In Neural Network ci sono troppi parametri che devo ottimizzare e questo può essere un problema perché può portare a overfitting. Ci sono poi anche parecchi hyperparameters. Essi includono il numero di layers che uso nel mio neural network, quanti neuroni metto in ogni layer, qual è la funzione di attivazione (posso scegliere una funzione diversa per ogni layer), qual è la connectivity di ogni layer (tutti gli imput values vanno in tutti i neuroni nel layer successivo?), un ulteriore hyperparameter è il numero di epoche.\n",
        "\n",
        "You want your loss function to get smaller.\n",
        "\n",
        "L'idea alla base è usare un modello fatto di lineare components per risolvere un problema che non è lineare. \n",
        "La scelta del punto in cui fermare il training del modello per prevenire overfitting è critica.\n",
        "\n",
        "Ogni nodo impara features diverse dai dati in imput.\n",
        "\n",
        "Il tempo computazionale è molto lungo.\n",
        "\n",
        "I Deep Neural Network sono combinazioni di modelli lineari.\n",
        "\n",
        "\n",
        "I neural netorks sono complessi modelli che mettono insieme ripetizioni di un'unità molto semplice, il perceptrons, una linear regression con un'activation function che trasforma l'output della linear regression in un output con range tra 0 e 1. I perceptrons sono i neuroni alla base, che svolgono un'operazione di moltiplicazione dell'input che gli viene dato + un bias. Quando mettiamo insieme molti neuroni in uno stesso layer e anche molti layer uno dopo l'altro possiamo approssimare fz complicate tranne queste unità semplici. Se l'input layer ha dimensionalità alta questa diventa una multilinear regression quindi h aun numero di coefficienti che corrisponde al numero di dati in input (che può essere molto alto). \n",
        "\n",
        "\n",
        "La dimensionalità del parametro permette di non rimanere incastrati in un minimo locale nella likelihood surface poiché non esistono minimi locali. La prob di avere un punto in cui tutti i parametri sono minimi è bassissima. Puoi sempre uscire da un local minimum da una direzione che non è minima.  \n",
        "\n",
        "Si deve scegliere la dimensione dei layers sulla base nel numero di oggetti in imput e in output (alla base c'è una moltiplicazione matriciale).\n",
        "\n",
        "**Backpropagation** posso passare l'errore sulla mia predizione ai miei neuroni sicché si possano adattare al risultato e ottenere il risultato corretto. Imparò più dai layer vicini al risultato che dai layer iniziali vicino all'imput."
      ]
    }
  ]
}